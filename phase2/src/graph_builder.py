"""
Phase 2: Static Graph Builder
==============================
ì •ì  ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¶• ë° í”¼ì²˜ ìƒì„±

í”¼ì²˜ êµ¬ì¡° (73ì°¨ì› - ê°„ì†Œí™” ë²„ì „):
- ì¬ë¬´ (4): ë§¤ì¶œ, ìˆ˜ì¶œ, ìì‚°, ìˆ˜ì¶œ/ë§¤ì¶œ ë¹„ìœ¨
- ì§€ë¦¬ (2): Xì¢Œí‘œ, Yì¢Œí‘œ
- ë¦¬ìŠ¤í¬ (1): TIS ì ìˆ˜
- ì‚°ì—… (33): IO ëŒ€ë¶„ë¥˜ ì›-í•« ì¸ì½”ë”©
- ë ˆì‹œí”¼ (33): Phase 1 ì¶œë ¥
"""

import numpy as np
import pandas as pd
import torch
import pickle
from pathlib import Path
from scipy.sparse import load_npz
from sklearn.preprocessing import StandardScaler
import logging

logger = logging.getLogger(__name__)


class StaticGraphBuilder:
    """
    ì •ì  ê·¸ë˜í”„ ë°ì´í„° ë¹Œë”
    
    Phase 1 ë ˆì‹œí”¼ + ì¬ë¬´/TIS ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬
    GraphSAGE í•™ìŠµìš© í”¼ì²˜ í–‰ë ¬ ìƒì„±
    """
    
    def __init__(self, data_dir: str, use_cache: bool = True):
        self.data_dir = Path(data_dir)
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"
        self.use_cache = use_cache
        
        # ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = self.processed_dir / "cache"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # IO ì‚°ì—… ì½”ë“œ (33ê°œ)
        self.io_sectors = ['A', 'B', 'C01', 'C02', 'C03', 'C04', 'C05', 'C06', 'C07', 'C08', 'C09', 
                           'C10', 'C11', 'C12', 'C13', 'C14', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 
                           'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T']
        self.io_sector_to_idx = {sec: i for i, sec in enumerate(self.io_sectors)}
        
    def build_static_data(self, use_simple_features: bool = True):
        """
        ì •ì  ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¶•
        
        Parameters
        ----------
        use_simple_features : bool
            True: 73ì°¨ì› (ê°„ì†Œí™”), False: 197ì°¨ì› (ì „ì²´)
        
        Returns
        -------
        X : np.ndarray [N, D]
            ë…¸ë“œ í”¼ì²˜ í–‰ë ¬
        edge_index : torch.Tensor [2, E]
            ì—£ì§€ ì¸ë±ìŠ¤
        edge_attr : torch.Tensor [E, edge_dim]
            ì—£ì§€ ì†ì„± (ê±°ë˜ì•¡)
        firm_ids : list
            ê¸°ì—… ID ë¦¬ìŠ¤íŠ¸
        """
        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        cache_suffix = "simple" if use_simple_features else "full"
        cache_files = {
            'X': self.cache_dir / f"static_X_{cache_suffix}.npy",
            'edge_index': self.cache_dir / "static_edge_index.pt",
            'edge_attr': self.cache_dir / "static_edge_attr.pt",
            'firm_ids': self.cache_dir / "static_firm_ids.pkl"
        }
        
        # ìºì‹œ í™•ì¸
        if self.use_cache and all(f.exists() for f in cache_files.values()):
            logger.info("=" * 70)
            logger.info("ğŸ“¦ ìºì‹œëœ ì •ì  ê·¸ë˜í”„ ë°ì´í„° ë¡œë“œ")
            logger.info("=" * 70)
            
            X = np.load(cache_files['X'])
            edge_index = torch.load(cache_files['edge_index'])
            edge_attr = torch.load(cache_files['edge_attr'])
            with open(cache_files['firm_ids'], 'rb') as f:
                firm_ids = pickle.load(f)
            
            logger.info(f"   âœ“ ë…¸ë“œ ìˆ˜: {len(firm_ids):,}")
            logger.info(f"   âœ“ í”¼ì²˜ ì°¨ì›: {X.shape[1]}")
            logger.info(f"   âœ“ ì—£ì§€ ìˆ˜: {edge_index.shape[1]:,}")
            logger.info("=" * 70)
            
            return X, edge_index, edge_attr, firm_ids
        
        # ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        logger.info("=" * 70)
        logger.info("ğŸ“Š ì •ì  ê·¸ë˜í”„ ë°ì´í„° êµ¬ì¶•")
        logger.info("=" * 70)
        
        # 1. Firm ID ë§¤í•‘ ë¡œë“œ
        logger.info("1ï¸âƒ£ ê¸°ì—… ì¸ë±ìŠ¤ ë§¤í•‘ ë¡œë“œ...")
        firm_to_idx_path = self.raw_dir / "firm_to_idx_model2.csv"
        df_idx = pd.read_csv(firm_to_idx_path)
        
        # ì»¬ëŸ¼ëª… í™•ì¸
        if 'Unnamed: 0' in df_idx.columns:
            firm_ids = df_idx['Unnamed: 0'].astype(str).tolist()
        elif 'ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸' in df_idx.columns:
            firm_ids = df_idx['ì‚¬ì—…ìë“±ë¡ë²ˆí˜¸'].astype(str).tolist()
        else:
            firm_ids = df_idx.iloc[:, 0].astype(str).tolist()
        
        N = len(firm_ids)
        logger.info(f"   âœ“ ê¸°ì—… ìˆ˜: {N:,}")
        
        # 2. H í–‰ë ¬ (ê±°ë˜ ë„¤íŠ¸ì›Œí¬) ë¡œë“œ
        logger.info("2ï¸âƒ£ H í–‰ë ¬ (ê±°ë˜ ë„¤íŠ¸ì›Œí¬) ë¡œë“œ...")
        H_path = self.raw_dir / "H_csr_model2.npz"
        H_sparse = load_npz(H_path)
        
        # Sparse â†’ PyTorch edge_index
        edge_index, edge_attr = self._sparse_to_edge_index(H_sparse)
        logger.info(f"   âœ“ ì—£ì§€ ìˆ˜: {edge_index.shape[1]:,}")
        logger.info(f"   âœ“ í‰ê·  ê±°ë˜ì•¡: {edge_attr.mean():.2e}")
        
        # 3. í”¼ì²˜ ìƒì„±
        logger.info("3ï¸âƒ£ ë…¸ë“œ í”¼ì²˜ ìƒì„±...")
        if use_simple_features:
            X = self._build_simple_features(firm_ids, N)
            logger.info(f"   âœ“ í”¼ì²˜ ì°¨ì›: {X.shape[1]} (ê°„ì†Œí™” ë²„ì „)")
        else:
            X = self._build_full_features(firm_ids, N)
            logger.info(f"   âœ“ í”¼ì²˜ ì°¨ì›: {X.shape[1]} (ì „ì²´ ë²„ì „)")
        
        # ìºì‹œ ì €ì¥
        if self.use_cache:
            logger.info("4ï¸âƒ£ ìºì‹œ ì €ì¥...")
            np.save(cache_files['X'], X)
            torch.save(edge_index, cache_files['edge_index'])
            torch.save(edge_attr, cache_files['edge_attr'])
            with open(cache_files['firm_ids'], 'wb') as f:
                pickle.dump(firm_ids, f)
            logger.info(f"   âœ“ ìºì‹œ ì €ì¥: {self.cache_dir}")
        
        logger.info("=" * 70)
        
        return X, edge_index, edge_attr, firm_ids
    
    def _sparse_to_edge_index(self, H_sparse):
        """
        Sparse í–‰ë ¬ì„ PyTorch edge_indexë¡œ ë³€í™˜
        
        Returns
        -------
        edge_index : torch.Tensor [2, E]
        edge_attr : torch.Tensor [E, 1]
        """
        # COO formatìœ¼ë¡œ ë³€í™˜
        H_coo = H_sparse.tocoo()
        
        edge_index = torch.tensor(
            np.vstack([H_coo.row, H_coo.col]),
            dtype=torch.long
        )
        
        edge_attr = torch.tensor(
            H_coo.data,
            dtype=torch.float
        ).unsqueeze(1)  # [E] â†’ [E, 1]
        
        return edge_index, edge_attr
    
    def _build_simple_features(self, firm_ids: list, N: int) -> np.ndarray:
        """
        ê°„ì†Œí™” í”¼ì²˜ ìƒì„± (73ì°¨ì›)
        
        êµ¬ì¡°:
        - ì¬ë¬´ (4): ë§¤ì¶œ, ìˆ˜ì¶œ, ìì‚°, ìˆ˜ì¶œ/ë§¤ì¶œ ë¹„ìœ¨
        - ì§€ë¦¬ (2): Xì¢Œí‘œ, Yì¢Œí‘œ
        - ë¦¬ìŠ¤í¬ (1): TIS ì ìˆ˜
        - ì‚°ì—… (33): IO ëŒ€ë¶„ë¥˜ ì›-í•«
        - ë ˆì‹œí”¼ (33): Phase 1 ì¶œë ¥
        
        Returns
        -------
        X : np.ndarray [N, 73]
        """
        logger.info("   - ê°„ì†Œí™” í”¼ì²˜ ìƒì„± ì¤‘ (73ì°¨ì›)...")
        
        # 1. ì¬ë¬´ í”¼ì²˜ (4ì°¨ì›)
        financial_features = self._load_financial_features(firm_ids, N)
        
        # 2. ì§€ë¦¬ í”¼ì²˜ (2ì°¨ì›)
        geo_features = self._load_geo_features(firm_ids, N)
        
        # 3. TIS í”¼ì²˜ (1ì°¨ì›)
        tis_features = self._load_tis_features(firm_ids, N)
        
        # 4. ì‚°ì—… í”¼ì²˜ (33ì°¨ì›)
        industry_features = self._load_industry_features(firm_ids, N)
        
        # 5. ë ˆì‹œí”¼ í”¼ì²˜ (33ì°¨ì›)
        recipe_features = self._load_recipe_features(firm_ids, N)
        
        # ê²°í•©
        X = np.hstack([
            financial_features,  # 4
            geo_features,        # 2
            tis_features,        # 1
            industry_features,   # 33
            recipe_features      # 33
        ])
        
        logger.info(f"      âœ“ ì¬ë¬´: {financial_features.shape[1]}ì°¨ì›")
        logger.info(f"      âœ“ ì§€ë¦¬: {geo_features.shape[1]}ì°¨ì›")
        logger.info(f"      âœ“ TIS: {tis_features.shape[1]}ì°¨ì›")
        logger.info(f"      âœ“ ì‚°ì—…: {industry_features.shape[1]}ì°¨ì›")
        logger.info(f"      âœ“ ë ˆì‹œí”¼: {recipe_features.shape[1]}ì°¨ì›")
        logger.info(f"      âœ“ ì´í•©: {X.shape[1]}ì°¨ì›")
        
        return X
    
    def _build_full_features(self, firm_ids: list, N: int) -> np.ndarray:
        """
        ì „ì²´ í”¼ì²˜ ìƒì„± (197ì°¨ì›)
        
        TODO: í•„ìš”ì‹œ êµ¬í˜„
        """
        logger.warning("   âš ï¸  ì „ì²´ í”¼ì²˜ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê°„ì†Œí™” ë²„ì „ ì‚¬ìš©")
        return self._build_simple_features(firm_ids, N)
    
    def _load_financial_features(self, firm_ids: list, N: int) -> np.ndarray:
        """
        ì¬ë¬´ í”¼ì²˜ ë¡œë“œ (4ì°¨ì›)
        - ë§¤ì¶œ (tg_2024_final)
        - ìˆ˜ì¶œ (export_value)
        - ìì‚° (asset)
        - ìˆ˜ì¶œ/ë§¤ì¶œ ë¹„ìœ¨
        """
        features = np.zeros((N, 4))
        
        # ë§¤ì¶œ ë°ì´í„°
        revenue_path = self.raw_dir / "final_tg_2024_estimation.csv"
        if revenue_path.exists():
            df_rev = pd.read_csv(revenue_path, dtype=str)
            df_rev['ì—…ì²´ë²ˆí˜¸'] = df_rev['ì—…ì²´ë²ˆí˜¸'].astype(str)
            
            # ë§¤ì¶œ ì»¬ëŸ¼ ì°¾ê¸°
            rev_col = None
            for col in ['tg_2024_final', 'tg_2024', 'revenue']:
                if col in df_rev.columns:
                    rev_col = col
                    break
            
            if rev_col:
                rev_dict = dict(zip(df_rev['ì—…ì²´ë²ˆí˜¸'], pd.to_numeric(df_rev[rev_col], errors='coerce')))
                for i, fid in enumerate(firm_ids):
                    if fid in rev_dict:
                        features[i, 0] = rev_dict[fid]
        
        # ìˆ˜ì¶œ ë°ì´í„°
        export_path = self.raw_dir / "export_estimation_value_final.csv"
        if export_path.exists():
            df_exp = pd.read_csv(export_path, dtype=str)
            df_exp['ì—…ì²´ë²ˆí˜¸'] = df_exp['ì—…ì²´ë²ˆí˜¸'].astype(str)
            
            exp_col = 'export_value' if 'export_value' in df_exp.columns else df_exp.columns[1]
            exp_dict = dict(zip(df_exp['ì—…ì²´ë²ˆí˜¸'], pd.to_numeric(df_exp[exp_col], errors='coerce')))
            for i, fid in enumerate(firm_ids):
                if fid in exp_dict:
                    features[i, 1] = exp_dict[fid]
        
        # ìì‚° ë°ì´í„°
        asset_path = self.raw_dir / "asset_final_2024_6ì°¨.csv"
        if asset_path.exists():
            df_asset = pd.read_csv(asset_path, dtype=str)
            df_asset['ì—…ì²´ë²ˆí˜¸'] = df_asset['ì—…ì²´ë²ˆí˜¸'].astype(str)
            
            asset_col = None
            for col in ['ìì‚°ì¶”ì •_2024', 'asset', 'ìì‚°']:
                if col in df_asset.columns:
                    asset_col = col
                    break
            
            if asset_col:
                asset_dict = dict(zip(df_asset['ì—…ì²´ë²ˆí˜¸'], pd.to_numeric(df_asset[asset_col], errors='coerce')))
                for i, fid in enumerate(firm_ids):
                    if fid in asset_dict:
                        features[i, 2] = asset_dict[fid]
        
        # ìˆ˜ì¶œ/ë§¤ì¶œ ë¹„ìœ¨
        with np.errstate(divide='ignore', invalid='ignore'):
            features[:, 3] = np.where(features[:, 0] > 0, features[:, 1] / features[:, 0], 0)
        
        # ì •ê·œí™” (log1p)
        features[:, :3] = np.log1p(np.abs(features[:, :3]))
        
        # NaN ì²˜ë¦¬
        features = np.nan_to_num(features, 0.0)
        
        return features
    
    def _load_geo_features(self, firm_ids: list, N: int) -> np.ndarray:
        """
        ì§€ë¦¬ í”¼ì²˜ ë¡œë“œ (2ì°¨ì›)
        - Xì¶•POIì¢Œí‘œê°’
        - Yì¶•POIì¢Œí‘œê°’
        """
        features = np.zeros((N, 2))
        
        firm_info_path = self.raw_dir / "vat_20-24_company_list_w_companyinfo_nocutoff_v3_hyundaisteel_hj.csv"
        if firm_info_path.exists():
            df_firm = pd.read_csv(firm_info_path, dtype=str)
            
            # ì‚¬ì—…ìë²ˆí˜¸ ì»¬ëŸ¼ ì°¾ê¸°
            biz_col = None
            for col in df_firm.columns:
                if 'ì‚¬ì—…ì' in col and 'ë²ˆí˜¸' in col:
                    biz_col = col
                    break
            
            if biz_col:
                df_firm[biz_col] = df_firm[biz_col].astype(str)
                
                # ì¢Œí‘œ ì»¬ëŸ¼
                x_col = 'Xì¶•POIì¢Œí‘œê°’' if 'Xì¶•POIì¢Œí‘œê°’' in df_firm.columns else None
                y_col = 'Yì¶•POIì¢Œí‘œê°’' if 'Yì¶•POIì¢Œí‘œê°’' in df_firm.columns else None
                
                if x_col and y_col:
                    coord_dict = dict(zip(
                        df_firm[biz_col],
                        zip(
                            pd.to_numeric(df_firm[x_col], errors='coerce'),
                            pd.to_numeric(df_firm[y_col], errors='coerce')
                        )
                    ))
                    
                    for i, fid in enumerate(firm_ids):
                        if fid in coord_dict:
                            x, y = coord_dict[fid]
                            if pd.notna(x) and pd.notna(y):
                                features[i, 0] = x
                                features[i, 1] = y
        
        # NaN ì²˜ë¦¬
        features = np.nan_to_num(features, 0.0)
        
        return features
    
    def _load_tis_features(self, firm_ids: list, N: int) -> np.ndarray:
        """
        TIS ë¦¬ìŠ¤í¬ í”¼ì²˜ ë¡œë“œ (1ì°¨ì›)
        """
        features = np.zeros((N, 1))
        
        tis_path = self.raw_dir / "shock_after_P_v2.csv"
        if tis_path.exists():
            df_tis = pd.read_csv(tis_path, dtype=str)
            df_tis['ì—…ì²´ë²ˆí˜¸'] = df_tis['ì—…ì²´ë²ˆí˜¸'].astype(str) if 'ì—…ì²´ë²ˆí˜¸' in df_tis.columns else df_tis.iloc[:, 0].astype(str)
            
            # TIS ì»¬ëŸ¼ ì°¾ê¸°
            tis_col = None
            for col in ['tis_score', 'shock_score', 'TIS']:
                if col in df_tis.columns:
                    tis_col = col
                    break
            
            if tis_col is None:
                tis_col = df_tis.columns[1] if len(df_tis.columns) > 1 else df_tis.columns[0]
            
            tis_dict = dict(zip(df_tis.iloc[:, 0], pd.to_numeric(df_tis[tis_col], errors='coerce')))
            for i, fid in enumerate(firm_ids):
                if fid in tis_dict and pd.notna(tis_dict[fid]):
                    features[i, 0] = tis_dict[fid]
        
        # ì •ê·œí™” (0-1)
        if features.max() > 0:
            features = features / features.max()
        
        # NaN ì²˜ë¦¬
        features = np.nan_to_num(features, 0.0)
        
        # TIS ì €ì¥
        tis_save_path = self.processed_dir / "tis_score_normalized.npy"
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        np.save(tis_save_path, features)
        
        return features
    
    def _load_industry_features(self, firm_ids: list, N: int) -> np.ndarray:
        """
        ì‚°ì—… ë¶„ë¥˜ í”¼ì²˜ ë¡œë“œ (33ì°¨ì› ì›-í•«)
        - IOìƒí’ˆ_ë‹¨ì¼_ëŒ€ë¶„ë¥˜_ì½”ë“œ ê¸°ë°˜
        """
        features = np.zeros((N, 33))
        
        firm_info_path = self.raw_dir / "vat_20-24_company_list_w_companyinfo_nocutoff_v3_hyundaisteel_hj.csv"
        if firm_info_path.exists():
            df_firm = pd.read_csv(firm_info_path, dtype=str)
            
            # ì‚¬ì—…ìë²ˆí˜¸ ì»¬ëŸ¼
            biz_col = None
            for col in df_firm.columns:
                if 'ì‚¬ì—…ì' in col and 'ë²ˆí˜¸' in col:
                    biz_col = col
                    break
            
            if biz_col:
                df_firm[biz_col] = df_firm[biz_col].astype(str)
                
                # IO ìƒí’ˆ ì½”ë“œ ì»¬ëŸ¼
                io_col = 'IOìƒí’ˆ_ë‹¨ì¼_ëŒ€ë¶„ë¥˜_ì½”ë“œ'
                if io_col in df_firm.columns:
                    io_dict = dict(zip(df_firm[biz_col], df_firm[io_col].astype(str).str.strip()))
                    
                    for i, fid in enumerate(firm_ids):
                        if fid in io_dict:
                            io_code = io_dict[fid]
                            if io_code in self.io_sector_to_idx:
                                sector_idx = self.io_sector_to_idx[io_code]
                                features[i, sector_idx] = 1.0
        
        return features
    
    def _load_recipe_features(self, firm_ids: list, N: int) -> np.ndarray:
        """
        Phase 1 ë ˆì‹œí”¼ ë¡œë“œ (33ì°¨ì›)
        """
        features = np.zeros((N, 33))
        
        recipe_path = self.processed_dir / "disentangled_recipes.pkl"
        if recipe_path.exists():
            with open(recipe_path, 'rb') as f:
                recipe_dict = pickle.load(f)
            
            for i, fid in enumerate(firm_ids):
                if fid in recipe_dict:
                    recipe = recipe_dict[fid]
                    if isinstance(recipe, np.ndarray) and len(recipe) == 33:
                        features[i] = recipe
        else:
            logger.warning(f"   âš ï¸  Phase 1 ë ˆì‹œí”¼ íŒŒì¼ ì—†ìŒ: {recipe_path}")
        
        return features
    
    def save_features(self, X: np.ndarray):
        """í”¼ì²˜ í–‰ë ¬ ì €ì¥"""
        save_path = self.processed_dir / "X_feature_matrix.npy"
        self.processed_dir.mkdir(parents=True, exist_ok=True)
        np.save(save_path, X)
        logger.info(f"ğŸ’¾ í”¼ì²˜ í–‰ë ¬ ì €ì¥: {save_path}")
