"""
Curriculum Learning Trainer
============================
ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµì„ ì§€ì›í•˜ëŠ” GraphSAGE íŠ¸ë ˆì´ë„ˆ
"""

import torch
import torch.optim as optim
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import logging
from typing import Optional, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CurriculumTrainer:
    """
    ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ íŠ¸ë ˆì´ë„ˆ
    
    Parameters
    ----------
    model : torch.nn.Module
        GraphSAGE ëª¨ë¸
    loss_fn : torch.nn.Module
        ì†ì‹¤ í•¨ìˆ˜
    optimizer : torch.optim.Optimizer
        ì˜µí‹°ë§ˆì´ì €
    device : str
        'cuda' or 'cpu'
    """
    
    def __init__(
        self,
        model: torch.nn.Module,
        loss_fn: torch.nn.Module,
        optimizer: torch.optim.Optimizer,
        device: str = 'cpu'
    ):
        self.model = model.to(device)
        self.loss_fn = loss_fn
        self.optimizer = optimizer
        self.device = device
        
        self.train_losses = []
        self.val_losses = []
    
    def train_epoch(
        self,
        x: torch.Tensor,
        edge_index: torch.Tensor,
        pos_edges: torch.Tensor,
        neg_edges: torch.Tensor,
        tis_scores: Optional[torch.Tensor] = None,
        batch_size: int = 1024
    ) -> dict:
        """
        1 ì—í­ í•™ìŠµ
        
        Parameters
        ----------
        x : torch.Tensor, shape (N, D)
            ë…¸ë“œ í”¼ì²˜
        edge_index : torch.Tensor, shape (2, E)
            ì „ì²´ ê·¸ëž˜í”„ ì—£ì§€ (message passingìš©)
        pos_edges : torch.Tensor, shape (2, P)
            Positive ì—£ì§€
        neg_edges : torch.Tensor, shape (2, N)
            Negative ì—£ì§€
        tis_scores : torch.Tensor, shape (N,), optional
            ë…¸ë“œë³„ TIS ì ìˆ˜
        batch_size : int
            ë°°ì¹˜ í¬ê¸°
        
        Returns
        -------
        metrics : dict
            - loss: í‰ê·  ì†ì‹¤
            - pos_score: Positive í‰ê·  ì ìˆ˜
            - neg_score: Negative í‰ê·  ì ìˆ˜
        """
        self.model.train()
        
        # ë°ì´í„°ë¥¼ deviceë¡œ ì´ë™
        x = x.to(self.device)
        edge_index = edge_index.to(self.device)
        pos_edges = pos_edges.to(self.device)
        neg_edges = neg_edges.to(self.device)
        
        if tis_scores is not None:
            tis_scores = tis_scores.to(self.device)
        
        total_loss = 0
        num_batches = 0
        all_pos_scores = []
        all_neg_scores = []
        
        # ì—£ì§€ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• 
        num_pos = pos_edges.shape[1]
        num_neg = neg_edges.shape[1]
        
        indices = np.arange(max(num_pos, num_neg))
        np.random.shuffle(indices)
        
        for start in range(0, len(indices), batch_size):
            end = min(start + batch_size, len(indices))
            batch_indices = indices[start:end]
            
            # Positive ë°°ì¹˜
            pos_batch_idx = batch_indices % num_pos
            pos_batch = pos_edges[:, pos_batch_idx]
            
            # Negative ë°°ì¹˜
            neg_batch_idx = batch_indices % num_neg
            neg_batch = neg_edges[:, neg_batch_idx]
            
            # Forward pass (ì „ì²´ ê·¸ëž˜í”„ë¡œ ìž„ë² ë”© ìƒì„±)
            self.optimizer.zero_grad()
            embeddings = self.model(x, edge_index)
            
            # Positive ì˜ˆì¸¡
            pos_pred = self.model.predict_link(embeddings, pos_batch)
            
            # Negative ì˜ˆì¸¡
            neg_pred = self.model.predict_link(embeddings, neg_batch)
            
            # ë ˆì´ë¸” ìƒì„±
            pos_labels = torch.ones_like(pos_pred)
            neg_labels = torch.zeros_like(neg_pred)
            
            # ì˜ˆì¸¡ ë° ë ˆì´ë¸” ê²°í•©
            pred = torch.cat([pos_pred, neg_pred])
            labels = torch.cat([pos_labels, neg_labels])
            
            # TIS ì ìˆ˜ (Positiveë§Œ)
            if tis_scores is not None:
                # Positive ì—£ì§€ì˜ ë„ì°© ë…¸ë“œ TIS
                pos_dst_tis = tis_scores[pos_batch[1]]
                batch_tis = torch.cat([pos_dst_tis, torch.zeros_like(neg_pred)])
            else:
                batch_tis = None
            
            # ì†ì‹¤ ê³„ì‚°
            loss = self.loss_fn(pred, labels, batch_tis)
            
            # Backward
            loss.backward()
            self.optimizer.step()
            
            # í†µê³„
            total_loss += loss.item()
            num_batches += 1
            all_pos_scores.append(pos_pred.detach().cpu().numpy())
            all_neg_scores.append(neg_pred.detach().cpu().numpy())
        
        # í‰ê·  ê³„ì‚°
        avg_loss = total_loss / num_batches
        avg_pos_score = np.concatenate(all_pos_scores).mean()
        avg_neg_score = np.concatenate(all_neg_scores).mean()
        
        return {
            'loss': avg_loss,
            'avg_pos_score': avg_pos_score,
            'avg_neg_score': avg_neg_score
        }
    
    @torch.no_grad()
    def evaluate(
        self,
        x: torch.Tensor,
        edge_index: torch.Tensor,
        pos_edges: torch.Tensor,
        neg_edges: torch.Tensor,
        batch_size: int = 2048
    ) -> dict:
        """
        í‰ê°€
        
        Returns
        -------
        metrics : dict
            - loss: í‰ê·  ì†ì‹¤
            - accuracy: ì •í™•ë„
            - pos_score: Positive í‰ê·  ì ìˆ˜
            - neg_score: Negative í‰ê·  ì ìˆ˜
        """
        self.model.eval()
        
        x = x.to(self.device)
        edge_index = edge_index.to(self.device)
        pos_edges = pos_edges.to(self.device)
        neg_edges = neg_edges.to(self.device)
        
        # ìž„ë² ë”© ìƒì„±
        embeddings = self.model(x, edge_index)
        
        # ë°°ì¹˜ í‰ê°€
        pos_scores = []
        neg_scores = []
        
        for start in range(0, pos_edges.shape[1], batch_size):
            end = min(start + batch_size, pos_edges.shape[1])
            batch_pos = pos_edges[:, start:end]
            pred = self.model.predict_link(embeddings, batch_pos)
            pos_scores.append(pred.cpu().numpy())
        
        for start in range(0, neg_edges.shape[1], batch_size):
            end = min(start + batch_size, neg_edges.shape[1])
            batch_neg = neg_edges[:, start:end]
            pred = self.model.predict_link(embeddings, batch_neg)
            neg_scores.append(pred.cpu().numpy())
        
        pos_scores = np.concatenate(pos_scores)
        neg_scores = np.concatenate(neg_scores)
        
        # ì •í™•ë„ ê³„ì‚°
        pos_correct = (pos_scores > 0.5).sum()
        neg_correct = (neg_scores <= 0.5).sum()
        accuracy = (pos_correct + neg_correct) / (len(pos_scores) + len(neg_scores))
        
        return {
            'accuracy': accuracy,
            'avg_pos_score': pos_scores.mean(),
            'avg_neg_score': neg_scores.mean()
        }
    
    def train(
        self,
        x: torch.Tensor,
        edge_index: torch.Tensor,
        train_pos_edges: torch.Tensor,
        val_pos_edges: torch.Tensor,
        sampler,
        epochs: int = 20,
        batch_size: int = 1024,
        tis_scores: Optional[torch.Tensor] = None,
        val_ratio: float = 0.2
    ):
        """
        ì „ì²´ í•™ìŠµ ë£¨í”„ (ì»¤ë¦¬í˜ëŸ¼)
        
        Parameters
        ----------
        sampler : CurriculumNegativeSampler
            ë„¤ê±°í‹°ë¸Œ ìƒ˜í”ŒëŸ¬
        """
        logger.info("=" * 70)
        logger.info("ðŸš€ ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ ì‹œìž‘")
        logger.info("=" * 70)
        
        for epoch in range(1, epochs + 1):
            # ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ (ì»¤ë¦¬í˜ëŸ¼)
            num_train_pos = train_pos_edges.shape[1]
            train_neg_edges, _ = sampler.sample(
                num_samples=num_train_pos * 2,  # 1:2 ë¹„ìœ¨ (í•™ìŠµ ì†ë„ í–¥ìƒ)
                epoch=epoch,
                total_epochs=epochs
            )
            train_neg_edges = train_neg_edges.to(self.device)
            
            # í•™ìŠµ
            train_metrics = self.train_epoch(
                x, edge_index, train_pos_edges, train_neg_edges,
                tis_scores=tis_scores, batch_size=batch_size
            )
            
            # ê²€ì¦ (ê°„ë‹¨ížˆ ëžœë¤ ë„¤ê±°í‹°ë¸Œ)
            val_neg_edges, _ = sampler.sample(
                num_samples=val_pos_edges.shape[1],
                epoch=1,  # Random only
                total_epochs=epochs
            )
            val_metrics = self.evaluate(
                x, edge_index, val_pos_edges, val_neg_edges, batch_size=batch_size
            )
            
            # ë¡œê¹…
            logger.info(
                f"Epoch {epoch:02d}/{epochs} | "
                f"Loss: {train_metrics['loss']:.4f} | "
                f"Pos: {train_metrics['avg_pos_score']:.3f} | "
                f"Neg: {train_metrics['avg_neg_score']:.3f} | "
                f"Val Acc: {val_metrics['accuracy']:.3f}"
            )
            
            self.train_losses.append(train_metrics['loss'])
            self.val_losses.append(val_metrics['accuracy'])
        
        logger.info("=" * 70)
        logger.info("âœ… í•™ìŠµ ì™„ë£Œ!")
        logger.info("=" * 70)


if __name__ == "__main__":
    print("Trainer ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ")
