"""
Phase 3: Hybrid Loss Function
==============================
TIS Í∏∞Î∞ò Soft Label BCE + Ranking Loss

ÏÜêÏã§ Ìï®Ïàò:
    1. TIS-aware BCE: Positive Ïó£ÏßÄÎäî 1.0 - TIS*alpha, NegativeÎäî 0.0 (ÎòêÎäî soft 0.05)
    2. Ranking Loss: Positive Ï†êÏàòÍ∞Ä Negative Ï†êÏàòÎ≥¥Îã§ marginÎßåÌÅº ÎÜíÏïÑÏïº Ìï®
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional


class HybridLoss(nn.Module):
    """
    Hybrid Loss = TIS-aware BCE + Ranking Loss
    
    Parameters
    ----------
    alpha : float
        TIS ÌéòÎÑêÌã∞ Í∞ïÎèÑ (default: 0.3)
    soft_negative : float
        Negative Ïó£ÏßÄÏùò Soft Label (default: 0.0)
    ranking_margin : float
        Ranking LossÏùò margin (default: 0.5)
    ranking_weight : float
        Ranking LossÏùò Í∞ÄÏ§ëÏπò (default: 0.1)
    """
    
    def __init__(
        self,
        alpha: float = 0.3,
        soft_negative: float = 0.0,
        ranking_margin: float = 0.5,
        ranking_weight: float = 0.1
    ):
        super().__init__()
        
        self.alpha = alpha
        self.soft_negative = soft_negative
        self.ranking_margin = ranking_margin
        self.ranking_weight = ranking_weight
        
        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')
        self.ranking_loss = nn.MarginRankingLoss(margin=ranking_margin, reduction='mean')
    
    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        tis_scores: Optional[torch.Tensor] = None,
        edge_index: Optional[torch.Tensor] = None
    ) -> tuple:
        """
        Parameters
        ----------
        logits : torch.Tensor [N]
            Î™®Îç∏ ÏòàÏ∏° (logit)
        labels : torch.Tensor [N]
            Ïã§Ï†ú Î†àÏù¥Î∏î (1.0 or 0.0)
        tis_scores : torch.Tensor [num_nodes], optional
            Í∞Å ÎÖ∏ÎìúÏùò TIS Ï†êÏàò
        edge_index : torch.Tensor [2, N], optional
            Ïó£ÏßÄ Ïù∏Îç±Ïä§ (tis_scores ÏÇ¨Ïö© Ïãú ÌïÑÏöî)
        
        Returns
        -------
        total_loss : torch.Tensor
        bce_loss : torch.Tensor
        ranking_loss : torch.Tensor
        """
        
        # ============================================================
        # 1. TIS Í∏∞Î∞ò Soft Label ÏÉùÏÑ±
        # ============================================================
        
        if tis_scores is not None and edge_index is not None:
            # edge_index: [2, batch_size]
            # tis_scores: [num_nodes] Ï†ÑÏ≤¥ ÎÖ∏ÎìúÏùò TIS
            
            # Positive Ïó£ÏßÄÏùò TIS Ï†êÏàò (dst ÎÖ∏Îìú Í∏∞Ï§Ä)
            dst_nodes = edge_index[1]  # [batch_size]
            
            # tis_scoresÍ∞Ä 1Ï∞®ÏõêÏù∏ÏßÄ ÌôïÏù∏
            if tis_scores.dim() == 1:
                dst_tis = tis_scores[dst_nodes]  # [batch_size]
            else:
                # 2Ï∞®ÏõêÏù∏ Í≤ΩÏö∞ squeeze
                dst_tis = tis_scores.squeeze()[dst_nodes]  # [batch_size]
            
            # Soft Label Í≥ÑÏÇ∞ (Î™ÖÏãúÏ†ÅÏúºÎ°ú 1D ÌÖêÏÑú ÏÉùÏÑ±)
            positive_labels = torch.ones_like(labels) - self.alpha * dst_tis
            negative_labels = torch.full_like(labels, self.soft_negative)
            
            soft_labels = torch.where(
                labels > 0.5,  # PositiveÏù∏ Í≤ΩÏö∞
                positive_labels,
                negative_labels
            )
        else:
            # TIS ÏóÜÏúºÎ©¥ ÏùºÎ∞ò Î†àÏù¥Î∏î
            soft_labels = torch.where(
                labels > 0.5,
                torch.ones_like(labels),
                torch.full_like(labels, self.soft_negative)
            )
        
        # ============================================================
        # 2. BCE Loss (Soft Label)
        # ============================================================
        
        bce = self.bce_loss(logits, soft_labels)
        bce_mean = bce.mean()
        
        # ============================================================
        # 3. Ranking Loss (Positive vs Negative)
        # ============================================================
        
        # Positive/Negative Î∂ÑÎ¶¨
        pos_mask = labels > 0.5
        neg_mask = ~pos_mask
        
        pos_logits = logits[pos_mask]
        neg_logits = logits[neg_mask]
        
        # Ranking Loss Í≥ÑÏÇ∞ (PositiveÍ∞Ä NegativeÎ≥¥Îã§ Ïª§Ïïº Ìï®)
        if len(pos_logits) > 0 and len(neg_logits) > 0:
            # Ïßù ÎßûÏ∂îÍ∏∞ (Í∞ôÏùÄ Í∞úÏàòÎßåÌÅº ÏÉòÌîåÎßÅ)
            min_size = min(len(pos_logits), len(neg_logits))
            pos_sample = pos_logits[:min_size]
            neg_sample = neg_logits[:min_size]
            
            # target = 1 (posÍ∞Ä negÎ≥¥Îã§ Ïª§Ïïº Ìï®)
            target = torch.ones(min_size, device=logits.device)
            ranking = self.ranking_loss(pos_sample, neg_sample, target)
        else:
            ranking = torch.tensor(0.0, device=logits.device)
        
        # ============================================================
        # 4. Total Loss
        # ============================================================
        
        total_loss = bce_mean + self.ranking_weight * ranking
        
        return total_loss, bce_mean, ranking


class TISAwareBCELoss(nn.Module):
    """
    TIS-aware BCE Loss (Ranking Loss ÏóÜÎäî Î≤ÑÏ†Ñ)
    
    Parameters
    ----------
    alpha : float
        TIS ÌéòÎÑêÌã∞ Í∞ïÎèÑ
    soft_negative : float
        Negative Ïó£ÏßÄÏùò Soft Label
    """
    
    def __init__(self, alpha: float = 0.3, soft_negative: float = 0.0):
        super().__init__()
        
        self.alpha = alpha
        self.soft_negative = soft_negative
        self.bce_loss = nn.BCEWithLogitsLoss(reduction='mean')
    
    def forward(
        self,
        logits: torch.Tensor,
        labels: torch.Tensor,
        tis_scores: Optional[torch.Tensor] = None,
        edge_index: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Returns
        -------
        loss : torch.Tensor
        """
        
        # TIS Í∏∞Î∞ò Soft Label
        if tis_scores is not None and edge_index is not None:
            dst_nodes = edge_index[1]
            dst_tis = tis_scores[dst_nodes]
            
            soft_labels = torch.where(
                labels > 0.5,
                1.0 - self.alpha * dst_tis,
                torch.full_like(labels, self.soft_negative)
            )
        else:
            soft_labels = torch.where(
                labels > 0.5,
                torch.ones_like(labels),
                torch.full_like(labels, self.soft_negative)
            )
        
        return self.bce_loss(logits, soft_labels)


# ============================================================
# Ïú†Ìã∏Î¶¨Ìã∞ Ìï®Ïàò
# ============================================================

def compute_soft_labels(
    labels: torch.Tensor,
    tis_scores: torch.Tensor,
    edge_index: torch.Tensor,
    alpha: float = 0.3,
    soft_negative: float = 0.0
) -> torch.Tensor:
    """
    TIS Í∏∞Î∞ò Soft Label Í≥ÑÏÇ∞
    
    Parameters
    ----------
    labels : torch.Tensor [N]
        ÏõêÎ≥∏ Î†àÏù¥Î∏î (1.0 or 0.0)
    tis_scores : torch.Tensor [num_nodes]
        Í∞Å ÎÖ∏ÎìúÏùò TIS Ï†êÏàò
    edge_index : torch.Tensor [2, N]
        Ïó£ÏßÄ Ïù∏Îç±Ïä§
    alpha : float
        TIS ÌéòÎÑêÌã∞ Í∞ïÎèÑ
    soft_negative : float
        Negative Ïó£ÏßÄÏùò Soft Label
    
    Returns
    -------
    soft_labels : torch.Tensor [N]
    """
    
    dst_nodes = edge_index[1]
    dst_tis = tis_scores[dst_nodes]
    
    soft_labels = torch.where(
        labels > 0.5,
        1.0 - alpha * dst_tis,  # Positive: 1.0 - TIS*alpha
        torch.full_like(labels, soft_negative)  # Negative: 0.0 or 0.05
    )
    
    return soft_labels


def visualize_soft_labels(
    labels: torch.Tensor,
    soft_labels: torch.Tensor,
    tis_scores: torch.Tensor,
    edge_index: torch.Tensor
):
    """
    Soft Label Î∂ÑÌè¨ ÏãúÍ∞ÅÌôî (ÎîîÎ≤ÑÍπÖÏö©)
    """
    import matplotlib.pyplot as plt
    
    pos_mask = labels > 0.5
    pos_soft = soft_labels[pos_mask].cpu().numpy()
    
    dst_nodes = edge_index[1][pos_mask]
    pos_tis = tis_scores[dst_nodes].cpu().numpy()
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 4))
    
    # Soft Label Î∂ÑÌè¨
    axes[0].hist(pos_soft, bins=50, alpha=0.7, color='blue', edgecolor='black')
    axes[0].set_xlabel('Soft Label (Positive)')
    axes[0].set_ylabel('Count')
    axes[0].set_title('Positive Edge Soft Label Distribution')
    axes[0].axvline(pos_soft.mean(), color='red', linestyle='--', label=f'Mean: {pos_soft.mean():.3f}')
    axes[0].legend()
    axes[0].grid(alpha=0.3)
    
    # TIS vs Soft Label
    axes[1].scatter(pos_tis, pos_soft, alpha=0.5, s=10)
    axes[1].set_xlabel('TIS Score (Destination)')
    axes[1].set_ylabel('Soft Label')
    axes[1].set_title('TIS Score vs Soft Label')
    axes[1].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('results/soft_label_distribution.png', dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"üìä Soft Label Î∂ÑÌè¨ Ï†ÄÏû•: results/soft_label_distribution.png")
    print(f"   - Positive Soft Label ÌèâÍ∑†: {pos_soft.mean():.4f}")
    print(f"   - Positive Soft Label Î≤îÏúÑ: [{pos_soft.min():.4f}, {pos_soft.max():.4f}]")
