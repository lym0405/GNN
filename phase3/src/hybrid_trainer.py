"""
Phase 3 Trainer: Two-Track Hybrid Learning
==========================================
Track A (TGN) + Track B (GraphSEAL) í†µí•© í•™ìŠµ
TIS-aware Soft Label + Ranking Loss ì ìš©
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from typing import Optional, Dict, List
import logging

from .loss import HybridLoss

logger = logging.getLogger(__name__)


class HybridTrainer:
    """
    Hybrid ëª¨ë¸ í•™ìŠµ (TGN + GraphSEAL)
    TIS-aware BCE + Ranking Loss ì‚¬ìš©
    
    [ìµœì í™”] Curriculum Learning ì§€ì›:
    - Phase 1: TGNë§Œ í•™ìŠµ (GraphSEAL ê³ ì •)
    - Phase 2: GraphSEALë§Œ í•™ìŠµ (TGN ê³ ì •)
    - Phase 3: ê²°í•© í•™ìŠµ (ë¯¸ì„¸ ì¡°ì •)
    """
    
    def __init__(
        self,
        hybrid_model: nn.Module,
        optimizer: optim.Optimizer,
        device: str = 'cpu',
        loss_alpha: float = 0.3,
        soft_negative: float = 0.0,
        ranking_weight: float = 0.1,
        curriculum_tgn_epochs: int = 5,
        curriculum_graphseal_epochs: int = 10
    ):
        self.model = hybrid_model.to(device)
        self.optimizer = optimizer
        self.device = device
        
        # Curriculum Learning ì„¤ì •
        self.curriculum_tgn_epochs = curriculum_tgn_epochs
        self.curriculum_graphseal_epochs = curriculum_graphseal_epochs
        self.current_epoch = 0  # í˜„ì¬ ì—í­ ì¶”ì 
        
        # Hybrid Loss (TIS-aware BCE + Ranking Loss)
        self.criterion = HybridLoss(
            alpha=loss_alpha,
            soft_negative=soft_negative,
            ranking_margin=0.5,
            ranking_weight=ranking_weight
        )
        
        # í•™ìŠµ ê¸°ë¡
        self.train_losses = []
        self.val_losses = []
        self.val_recalls = []
        
        self.best_val_recall = 0.0
        self.patience_counter = 0
    
    def train_epoch(
        self,
        events: List,
        node_features: torch.Tensor,
        node_embeddings: torch.Tensor,
        edge_index: torch.Tensor,
        tis_scores: Optional[torch.Tensor] = None,
        batch_size: int = 1024
    ) -> float:
        """
        1 ì—í­ í•™ìŠµ
        
        [ìµœì í™”] Curriculum Learning ì ìš©:
        - ì´ˆê¸°: TGNë§Œ í•™ìŠµ (ë¹ ë¥¸ ìˆ˜ë ´)
        - ì¤‘ë°˜: GraphSEALë§Œ í•™ìŠµ (êµ¬ì¡° íŒ¨í„´ í•™ìŠµ)
        - í›„ë°˜: ê²°í•© í•™ìŠµ (ë¯¸ì„¸ ì¡°ì •)
        
        Parameters
        ----------
        events : List of (timestamp, src, dst, edge_feat, label)
        node_features : [N, node_dim]
        node_embeddings : [N, emb_dim] (Phase 2 ì¶œë ¥)
        edge_index : [2, E] (ì „ì²´ ê·¸ë˜í”„)
        tis_scores : [N] (ë…¸ë“œë³„ TIS)
        batch_size : int
        
        Returns
        -------
        avg_loss : float
        """
        self.model.train()
        
        # [ìµœì í™”] Curriculum Learning: í˜„ì¬ ì—í­ì— ë”°ë¼ í•™ìŠµ ëª¨ë“œ ê²°ì •
        if self.current_epoch < self.curriculum_tgn_epochs:
            training_mode = 'tgn_only'
            logger.info(f"  ğŸ“š Curriculum Learning: TGN Only (Epoch {self.current_epoch + 1})")
        elif self.current_epoch < self.curriculum_tgn_epochs + self.curriculum_graphseal_epochs:
            training_mode = 'graphseal_only'
            logger.info(f"  ğŸ“š Curriculum Learning: GraphSEAL Only (Epoch {self.current_epoch + 1})")
        else:
            training_mode = 'hybrid'
            logger.info(f"  ğŸ“š Curriculum Learning: Hybrid (Epoch {self.current_epoch + 1})")
        
        total_loss = 0.0
        num_batches = 0
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(events), batch_size):
            batch_events = events[i:i+batch_size]
            
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            timestamps = torch.tensor([e[0] for e in batch_events], dtype=torch.long)
            src_nodes = torch.tensor([e[1] for e in batch_events], dtype=torch.long)
            dst_nodes = torch.tensor([e[2] for e in batch_events], dtype=torch.long)
            edge_feats = torch.stack([torch.tensor(e[3]) for e in batch_events])
            labels = torch.tensor([e[4] for e in batch_events], dtype=torch.float32)
            
            # GPUë¡œ ì´ë™
            timestamps = timestamps.to(self.device)
            src_nodes = src_nodes.to(self.device)
            dst_nodes = dst_nodes.to(self.device)
            edge_feats = edge_feats.to(self.device)
            labels = labels.to(self.device)
            
            # ë…¸ë“œ í”¼ì²˜
            src_features = node_features[src_nodes].to(self.device)
            dst_features = node_features[dst_nodes].to(self.device)
            
            # [ìµœì í™”] Curriculum Learning: í•™ìŠµ ëª¨ë“œì— ë”°ë¼ ë¶„ê¸°
            if training_mode == 'tgn_only':
                # Phase 1: TGNë§Œ í•™ìŠµ (GraphSEALì€ gradient ê³„ì‚° ì•ˆ í•¨)
                # TGN Forward
                with torch.no_grad():
                    # GraphSEAL ë¶€ë¶„ì€ no_gradë¡œ ìŠ¤í‚µ (ì†ë„ í–¥ìƒ)
                    pass
                
                logits = self.model.tgn(
                    src_nodes=src_nodes,
                    dst_nodes=dst_nodes,
                    src_features=src_features,
                    dst_features=dst_features,
                    timestamps=timestamps
                )
                outputs = None
                
            elif training_mode == 'graphseal_only':
                # Phase 2: GraphSEALë§Œ í•™ìŠµ (TGNì€ ê³ ì •)
                with torch.no_grad():
                    # TGNì˜ ì¶œë ¥ì„ ê³ ì •í•˜ì—¬ ì‚¬ìš©
                    tgn_logits = self.model.tgn(
                        src_nodes=src_nodes,
                        dst_nodes=dst_nodes,
                        src_features=src_features,
                        dst_features=dst_features,
                        timestamps=timestamps
                    )
                
                # GraphSEAL Forward (DRNLë§Œ ì‚¬ìš©)
                logits = self.model.graphseal(
                    src_nodes=src_nodes,
                    dst_nodes=dst_nodes,
                    node_embeddings=node_embeddings.to(self.device),
                    edge_index=edge_index.to(self.device),
                    tis_scores=None
                )
                outputs = None
                
            else:
                # Phase 3: Hybrid (ì „ì²´ í•™ìŠµ)
                logits, outputs = self.model(
                    src_nodes=src_nodes,
                    dst_nodes=dst_nodes,
                    src_features=src_features,
                    dst_features=dst_features,
                    node_embeddings=node_embeddings.to(self.device),
                    edge_index=edge_index.to(self.device),
                    timestamps=timestamps,
                    tis_scores=None  # TISëŠ” lossì—ì„œ ì‚¬ìš©
                )
            
            # Loss (TIS-aware Soft Label + Ranking Loss)
            batch_edge_index = torch.stack([src_nodes, dst_nodes], dim=0)
            batch_tis = tis_scores.to(self.device) if tis_scores is not None else None
            
            total_loss_val, bce_loss, ranking_loss = self.criterion(
                logits=logits,
                labels=labels,
                tis_scores=batch_tis,
                edge_index=batch_edge_index
            )
            
            loss = total_loss_val
            
            # Backward
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # TGN ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ (Track A)
            with torch.no_grad():
                self.model.tgn.update_memory_with_batch(
                    src_nodes, dst_nodes, edge_feats, timestamps
                )
            
            total_loss += loss.item()
            num_batches += 1
        
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        
        # ì—í­ ì¹´ìš´í„° ì¦ê°€
        self.current_epoch += 1
        
        return avg_loss
    
    @torch.no_grad()
    def evaluate(
        self,
        events: List,
        node_features: torch.Tensor,
        node_embeddings: torch.Tensor,
        edge_index: torch.Tensor,
        tis_scores: Optional[torch.Tensor] = None,
        k_list: List[int] = [10, 50, 100],
        batch_size: int = 2048
    ) -> Dict:
        """
        í‰ê°€ (Recall@K ì¤‘ì‹¬)
        
        Returns
        -------
        metrics : Dict
            {
                'loss': float,
                'recall@10': float,
                'recall@50': float,
                ...
            }
        """
        self.model.eval()
        
        total_loss = 0.0
        num_batches = 0
        
        all_scores = []
        all_labels = []
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for i in range(0, len(events), batch_size):
            batch_events = events[i:i+batch_size]
            
            # ë°°ì¹˜ ë°ì´í„° ì¶”ì¶œ
            timestamps = torch.tensor([e[0] for e in batch_events], dtype=torch.long)
            src_nodes = torch.tensor([e[1] for e in batch_events], dtype=torch.long)
            dst_nodes = torch.tensor([e[2] for e in batch_events], dtype=torch.long)
            labels = torch.tensor([e[4] for e in batch_events], dtype=torch.float32)
            
            # GPUë¡œ ì´ë™
            timestamps = timestamps.to(self.device)
            src_nodes = src_nodes.to(self.device)
            dst_nodes = dst_nodes.to(self.device)
            labels = labels.to(self.device)
            
            # ë…¸ë“œ í”¼ì²˜
            src_features = node_features[src_nodes].to(self.device)
            dst_features = node_features[dst_nodes].to(self.device)
            
            # Forward
            logits, outputs = self.model(
                src_nodes=src_nodes,
                dst_nodes=dst_nodes,
                src_features=src_features,
                dst_features=dst_features,
                node_embeddings=node_embeddings.to(self.device),
                edge_index=edge_index.to(self.device),
                timestamps=timestamps,
                tis_scores=None
            )
            
            # Loss (TIS-aware)
            batch_edge_index = torch.stack([src_nodes, dst_nodes], dim=0)
            batch_tis = tis_scores.to(self.device) if tis_scores is not None else None
            
            total_loss_val, bce_loss, ranking_loss = self.criterion(
                logits=logits,
                labels=labels,
                tis_scores=batch_tis,
                edge_index=batch_edge_index
            )
            
            total_loss += total_loss_val.item()
            num_batches += 1
            
            # ì ìˆ˜ ì €ì¥
            scores = torch.sigmoid(logits)
            all_scores.append(scores.cpu().numpy())
            all_labels.append(labels.cpu().numpy())
        
        # ì „ì²´ ë°ì´í„° ê²°í•©
        all_scores = np.concatenate(all_scores)
        all_labels = np.concatenate(all_labels)
        
        # Recall@K ê³„ì‚°
        metrics = {'loss': total_loss / num_batches if num_batches > 0 else 0.0}
        
        for k in k_list:
            recall_k = self._compute_recall_at_k(all_scores, all_labels, k)
            metrics[f'recall@{k}'] = recall_k
        
        return metrics
    
    def _compute_recall_at_k(
        self,
        scores: np.ndarray,
        labels: np.ndarray,
        k: int
    ) -> float:
        """
        Recall@K ê³„ì‚°
        
        ìƒìœ„ Kê°œ ì˜ˆì¸¡ ì¤‘ ì‹¤ì œ Positiveê°€ ëª‡ ê°œ í¬í•¨ë˜ëŠ”ê°€?
        """
        # ìƒìœ„ Kê°œ ì¸ë±ìŠ¤
        top_k_indices = np.argsort(scores)[-k:]
        
        # Positive ê°œìˆ˜
        num_positives = labels.sum()
        
        if num_positives == 0:
            return 0.0
        
        # ìƒìœ„ Kê°œ ì¤‘ Positive ê°œìˆ˜
        num_hits = labels[top_k_indices].sum()
        
        # Recall
        recall = num_hits / num_positives
        
        return recall
    
    def train(
        self,
        train_events: List,
        val_events: List,
        node_features: torch.Tensor,
        node_embeddings: torch.Tensor,
        train_edge_index: torch.Tensor,
        tis_scores: Optional[torch.Tensor] = None,
        epochs: int = 50,
        batch_size: int = 1024,
        early_stopping_patience: int = 10,
        k_list: List[int] = [10, 50, 100],
        verbose: bool = True
    ):
        """
        ì „ì²´ í•™ìŠµ ë£¨í”„
        """
        logger.info("=" * 70)
        logger.info("ğŸš€ Hybrid Training ì‹œì‘ (TGN + GraphSEAL)")
        logger.info("=" * 70)
        
        for epoch in range(1, epochs + 1):
            # TGN ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ë§¤ ì—í­ë§ˆë‹¤)
            self.model.tgn.reset_memory()
            
            # í•™ìŠµ
            train_loss = self.train_epoch(
                train_events,
                node_features,
                node_embeddings,
                train_edge_index,
                tis_scores,
                batch_size
            )
            
            # ê²€ì¦
            val_metrics = self.evaluate(
                val_events,
                node_features,
                node_embeddings,
                train_edge_index,
                tis_scores,
                k_list,
                batch_size * 2
            )
            
            # ê¸°ë¡
            self.train_losses.append(train_loss)
            self.val_losses.append(val_metrics['loss'])
            self.val_recalls.append(val_metrics['recall@50'])
            
            # Early Stopping
            if val_metrics['recall@50'] > self.best_val_recall:
                self.best_val_recall = val_metrics['recall@50']
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            # ì¶œë ¥
            if verbose:
                logger.info(
                    f"Epoch {epoch:02d}/{epochs} | "
                    f"Train Loss: {train_loss:.4f} | "
                    f"Val Loss: {val_metrics['loss']:.4f} | "
                    f"Recall@10: {val_metrics['recall@10']:.4f} | "
                    f"Recall@50: {val_metrics['recall@50']:.4f} | "
                    f"Recall@100: {val_metrics['recall@100']:.4f}"
                )
            
            # Early Stopping ì²´í¬
            if self.patience_counter >= early_stopping_patience:
                logger.info(f"\nâš ï¸  Early Stopping at Epoch {epoch}")
                break
        
        logger.info("=" * 70)
        logger.info(f"âœ… í•™ìŠµ ì™„ë£Œ! Best Recall@50: {self.best_val_recall:.4f}")
        logger.info("=" * 70)
